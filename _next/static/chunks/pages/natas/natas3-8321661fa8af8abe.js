(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[497],{4708:function(e,t,o){(window.__NEXT_P=window.__NEXT_P||[]).push(["/natas/natas3",function(){return o(5405)}])},4655:function(e,t,o){"use strict";var s=o(5893),i=o(335),n=o.n(i);t.Z=function(e){var t=e.src,o=e.alt,i=e.width,a=void 0===i?600:i;return(0,s.jsx)("div",{className:n().imgContainer,children:(0,s.jsx)("img",{src:t,alt:o,width:a})})}},654:function(e,t,o){"use strict";var s=o(5893),i=o(3457),n=o.n(i),a=o(5227),r=o(4283);t.Z=function(e){var t=e.children,o=e.language;return(0,s.jsx)("div",{className:n().CodeBlock,children:(0,s.jsx)(a.Z,{language:o,style:r.cL,children:t})})}},9242:function(e,t,o){"use strict";var s=o(5893),i=o(1664),n=o(8574),a=o.n(n);t.Z=function(e){var t=e.link1,o=e.link2;return(0,s.jsxs)("div",{className:a().pageNav,children:[(0,s.jsx)("div",{children:t?(0,s.jsx)(i.default,{href:t,children:(0,s.jsx)("a",{title:"Previous page",className:a().navButton,children:"\u25c0"})}):""}),(0,s.jsx)("div",{children:o?(0,s.jsx)(i.default,{href:o,children:(0,s.jsx)("a",{title:"Next page",className:a().navButton,children:"\u25b6"})}):""})]})}},586:function(e,t,o){"use strict";var s=o(5893),i=o(6655),n=o.n(i),a=o(5227),r=o(4283),l=o(7294);t.Z=function(e){var t=e.children,o=(0,l.useState)(!1),i=o[0],c=o[1];return(0,s.jsxs)("section",{children:[(0,s.jsx)("h2",{children:"Key"}),(0,s.jsxs)("div",{className:n().codeBlock,children:[(0,s.jsx)("div",{className:n().spoilerToggler,children:(0,s.jsx)("button",{onClick:function(){return c(!i)},children:"Click to Reveal"})}),(0,s.jsx)("div",{className:n().spoilerContainer,children:(0,s.jsx)("div",{className:"".concat(n().spoiler," ").concat(i?n().show:""),children:(0,s.jsx)(a.Z,{language:"",style:r.cL,customStyle:{margin:0},children:t})})})]})]})}},5405:function(e,t,o){"use strict";o.r(t);var s=o(5893),i=o(4655),n=o(654),a=o(9242),r=o(586);t.default=function(){return(0,s.jsxs)("div",{children:[(0,s.jsx)("h1",{children:"Natas 3"}),(0,s.jsxs)("section",{children:[(0,s.jsx)("h2",{children:"Experience"}),(0,s.jsx)("p",{children:"Opening the developer tools, the page actually looked quite clean. There is nothing obvious except a snarky cheeky remark about how even Google can' find it. I had a look around the file structure and there was nothing there as well."}),(0,s.jsx)(i.Z,{alt:"The html page showing nothing much except a comment about how not even Google will find any information leak",src:"/natas3-1.png"}),(0,s.jsxs)("p",{children:['Without much luck, I tried using that clue, of not being able to be seen by Google, and searched "programming how to hide sometthing in website from google". I found the following article '," ",(0,s.jsx)("a",{href:"https://www.searchenginejournal.com/google-explains-how-to-hide-a-website-from-search-results/428341/",children:"How to Hide a Website from Search Result"})," which presented several ideas. The first method is to use a password, I don't see any links which required a password in the page, so it can't be that one. The second method mention blocking it from web crawlers by adding a robots.txt. So I appended robots.txt to the web address, and voila, was greeted with the text:"]})]}),(0,s.jsx)(n.Z,{language:"s",children:"User-agent: *\nDisallow: /s3cr3t/"}),(0,s.jsx)("p",{children:"So I appended this to the root address, and I found it to be a directory path with a users.txt which had the password within it."}),(0,s.jsxs)("section",{children:[(0,s.jsx)("h2",{children:"Research on robots.txt"}),(0,s.jsxs)("p",{children:["What is robots.txt? According to"," ",(0,s.jsx)("a",{href:"https://developers.google.com/search/docs/advanced/robots/intro",children:"Google Search Central"})," ","robots.txt tells web crawlers what website urls it is allowed to access. It is actually not as Google-proof as the comment stated it was. As Google Search Central state:"," ",(0,s.jsx)("q",{children:"it is not a mechanism for keeping a web page out of Google."})," The main problem is that while it tell webcrawlers of prohibited pages in a website, if the pages itself are linked from other websites, it is still possible for the page to be indexed by web crawlers. Google Search Central then points out to use"," ",(0,s.jsx)("a",{href:"https://developers.google.com/search/docs/advanced/crawling/block-indexing",children:"noindex"}),"."]}),(0,s.jsx)("p",{children:"So what about noindex? As described on the site, noindex is a meta data which you can include in a meta tag of a document or on a HTML request header. When this is read by web crawlers, it informs them to not index the page. Would that have made the webpage unaccessible? Nope, none of these would defend against malicious web crawlers."}),(0,s.jsxs)("p",{children:["While robots.txt is said to inform web crawlers of what urls it can access, this action still depends on the web crawler itself. A malicious web crawler could actually simply ignore the robots.txt and the noindex. So these are mainly used for ",(0,s.jsx)("em",{children:"Search Engine Optimization"})," (SEO), not for security purposes."]})]}),(0,s.jsxs)("section",{children:[(0,s.jsx)("h2",{children:"Reflection"}),(0,s.jsxs)("p",{children:["Overall, the challenge just informs us of features that may at first seem like security features, but which is in fact not. As website development is a massive field of study, there are bound to be time where we would search how to do certain things. This challenge has taught me to be extra careful at such times. In a rush, I may have not realised that robots.txt was not supposed to secure against malicious web crawlers. The first line of information about it on Google Search Central itself mention that it ",(0,s.jsx)("strong",{children:"is"})," used to tell web crawlers which urls it can access. The wording is quite vague and people could be misled if they did not read further on the page."]}),(0,s.jsx)("p",{children:"In general, I suppose, this encourages software developers to always read the documentations first. Also, as contributing software developer ourselves, we should be writing clearer documentations about the proper use of our technology and any warnings people should know. Doing so would help others in the community to create more secure programs. Another interesting thought is that, as developers, we should always checked if what we have implemented actually does what we intended it to do. By simply, adding robots.txt to the url, the developer could have easily notice the vulnerability."})]}),(0,s.jsx)(r.Z,{children:"Z9tkRkWmpt9Qr7XrR5jWRkgOU901swEZ"}),(0,s.jsx)(a.Z,{link1:"natas2",link2:"natas4"})]})}},335:function(e){e.exports={imgContainer:"BlogImage_imgContainer__s2tL6"}},3457:function(e){e.exports={CodeBlock:"CodeBlock_CodeBlock___IBBd"}},8574:function(e){e.exports={pageNav:"PageNav_pageNav__NXqbT",navButton:"PageNav_navButton__yHhmO"}},6655:function(e){e.exports={codeBlock:"SpoilerKey_codeBlock__KWl0d",spoilerToggler:"SpoilerKey_spoilerToggler__HCeWZ",spoiler:"SpoilerKey_spoiler__YWx_s",show:"SpoilerKey_show__VIJ75",spoilerContainer:"SpoilerKey_spoilerContainer__O76It"}}},function(e){e.O(0,[369,774,888,179],(function(){return t=4708,e(e.s=t);var t}));var t=e.O();_N_E=t}]);